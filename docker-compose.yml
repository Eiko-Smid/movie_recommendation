services:
  mlflow:
    build:
      context: .
      dockerfile: Dockerfile.mlflow
      args:
        MLFLOW_VERSION: "3.4.0"
    container_name: mlflow_server
    ports:
      - "5000:5000"
    # Single canonical volume where BOTH the DB and artifacts live on your host
    volumes:
      - ./mlflow:/mlflow


  # PostgreSQL Database
  postgres:
    image: postgres:15
    container_name: postgres_db
    restart: always
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 10s
      retries: 40
      start_period: 30s
    ports:
      - "5432:5432"
    volumes:
       # Persistent database files on host:
      - ./postgres_data:/var/lib/postgresql/data
      # Auto-restore on first init (runs only when data dir is empty):
      - ./data/dump:/docker-entrypoint-initdb.d:ro

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: movie_rec_api
    depends_on: 
      postgres:
            # Waits with start until db is set up
          condition: service_healthy
      mlflow:
        condition: service_started
    ports:
      - "8000:8000"   # FastAPI
      - "5678:5678"   # debugpy
    environment:
      # If you run MLflow on Windows host at :5000, use host.docker.internal
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      MLFLOW_EXPERIMENT_NAME: "als_movie_rec"
      MODEL_NAME: "als_model_versioning"
      OPENBLAS_NUM_THREADS: "1"
    volumes:
      # Mount your data exactly where code expects it
      - ./data:/app/data:ro
      # Optional: live-edit code without rebuilds
      - ./src:/app/src
      # Champion csr matrix storage
      - ./champ_store:/app/champ_store
      # Environment var for db.
      - ./.env:/app/.env

  # Add streamlit
  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    container_name: streamlit_app
    depends_on:
      - api      
    ports:
      - "8501:8501"
    environment:
      DB_URL: ${DB_URL}
      API_URL: "http://api:8000" 
    networks:
      - default 
    volumes:
      - ./streamlit:/app     # mount local streamlit folder inside container at /app

  # ðŸ• Scheduled Trainer (daily retraining)
  # Training can be manually started with: docker exec -it daily_trainer sh -c "curl -X POST http://api:8000/train -H 'Content-Type: application/json' -d @/data/train_payload.json
  trainer:
    image: curlimages/curl:8.5.0
    container_name: daily_trainer
    user: root  # âœ… allow writing to /etc/crontabs/root
    depends_on:
      api:
        condition: service_started
    command: >
      sh -c "
        echo \"* 2 * * * echo \\\"\$(date '+%Y-%m-%d %H:%M:%S') - Starting training...\\\" >> /var/log/cron.log && curl -X POST http://api:8000/train -H 'Content-Type: application/json' -d @/data/train_payload.json >> /var/log/cron.log 2>&1\" > /etc/crontabs/root &&
        echo 'âœ… Cron job scheduled: daily at 2:00 UTC -> POST http://api:8000/train using /data/train_payload.json' &&
        crond -f -l 2
      "
    volumes:
      - ./logs:/var/log  # persist cron logs locally
      - ./train_payload.json:/data/train_payload.json:ro  # âœ… mount payload file read-only